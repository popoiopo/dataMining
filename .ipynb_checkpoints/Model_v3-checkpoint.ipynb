{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nDCG(relevances: np.ndarray, positions: np.ndarray) -> float:\n",
    "    \"\"\"Compute Normalized Discounted Cumulative Gain based on:\n",
    "    - relevances: Numpy Array containing DCG Relevances (5 if booked, 1 if clicked)\n",
    "    - positions: Numpy Array containing Positions (The display order) \"\"\"\n",
    "    \n",
    "    positions_normalized = np.argsort(positions)\n",
    "    relevances_ordered = relevances[positions_normalized]\n",
    "    relevances_sorted = np.sort(relevances)[::-1]\n",
    "    \n",
    "    gain = 2 ** relevances_ordered - 1\n",
    "    ideal_gain = 2 ** relevances_sorted - 1\n",
    "    \n",
    "    discount = np.log2(np.arange(len(positions)) + 2)\n",
    "    \n",
    "    DCG = np.sum(gain / discount)\n",
    "    IDCG = np.sum(ideal_gain / discount)\n",
    "    \n",
    "    return DCG / IDCG\n",
    "\n",
    "\n",
    "def nDCG_mean(dataframe: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Mean Normalized Discounted Cumulative Gain on DataFrame\n",
    "    DataFrame must have fields: [srch_id, relevance, position]\n",
    "    \"\"\"\n",
    "    \n",
    "    nDCG_sum = 0.0\n",
    "    \n",
    "    searches = dataframe.groupby('srch_id')\n",
    "    \n",
    "    for name, search in searches:\n",
    "        nDCG_sum += nDCG(search.relevance.values, search.position.values)\n",
    "    return nDCG_sum / len(searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load That Shit (does take a while)\n",
    "df = pd.read_csv(\"data/training_set_VU_DM_2014.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Relevance Column to DataFrame\n",
    "relevance = np.zeros(len(df))\n",
    "relevance[df['click_bool'] == 1] = 1\n",
    "relevance[df['booking_bool'] == 1] = 5\n",
    "\n",
    "df['relevance'] = relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize DataFrame (a.k.a. throw out competitor info): handy for tweaking/overview?\n",
    "df = df[[\n",
    "    # ID\n",
    "    'srch_id',\n",
    "    \n",
    "    # Labels (to predict)\n",
    "    'position',\n",
    "    'relevance',\n",
    "    'click_bool',\n",
    "    'booking_bool',\n",
    "    \n",
    "    # Per Seach Features\n",
    "    'date_time',\n",
    "    'site_id',\n",
    "    'srch_destination_id',\n",
    "    'srch_length_of_stay',\n",
    "    'srch_booking_window',\n",
    "    'srch_adults_count',\n",
    "    'srch_children_count',\n",
    "    'srch_room_count',\n",
    "    'srch_saturday_night_bool',\n",
    "    'srch_query_affinity_score',\n",
    "    'orig_destination_distance',\n",
    "    \n",
    "    # Property Features\n",
    "    'price_usd',\n",
    "    'promotion_flag',\n",
    "    'prop_country_id',\n",
    "    'prop_id',\n",
    "    'prop_starrating',\n",
    "    'prop_review_score',\n",
    "    'prop_brand_bool',\n",
    "    'prop_location_score1',\n",
    "    'prop_location_score2',\n",
    "    'prop_log_historical_price',\n",
    "    \n",
    "    # Visitor Features\n",
    "    'visitor_location_country_id',\n",
    "    'visitor_hist_starrating',\n",
    "    'visitor_hist_adr_usd',\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Month/Day/Hour from Datetime\n",
    "dates = pd.DatetimeIndex(df['date_time'])\n",
    "\n",
    "df['month'] = dates.month\n",
    "df['day'] = dates.day\n",
    "df['hour'] = dates.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight Matching Data\n",
    "df['starrating_diff'] = df.visitor_hist_starrating - df.prop_starrating\n",
    "df['country_id_match'] = df.visitor_location_country_id == df.prop_country_id\n",
    "df['user_usd_diff'] = df.price_usd - df.visitor_hist_adr_usd\n",
    "df['has_booked_before'] = df.visitor_hist_adr_usd.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.srch_query_affinity_score = df.srch_query_affinity_score.fillna(df.srch_query_affinity_score.mean())\n",
    "df.orig_destination_distance = df.orig_destination_distance.fillna(df.orig_destination_distance.mean())\n",
    "\n",
    "df.prop_location_score2 = df.prop_location_score2.fillna(df.prop_location_score2.min())\n",
    "\n",
    "df.visitor_hist_adr_usd = df.visitor_hist_adr_usd.fillna(-1)\n",
    "df.user_usd_diff = df.user_usd_diff.fillna(-1)\n",
    "df.visitor_hist_starrating = df.visitor_hist_starrating.fillna(-1)\n",
    "df.starrating_diff = df.starrating_diff.fillna(0)\n",
    "\n",
    "df.prop_review_score = df.prop_review_score.fillna(df.prop_review_score.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srch_id                                           0.0% \n",
      "position                                          0.0% \n",
      "relevance                                         0.0% \n",
      "click_bool                                        0.0% \n",
      "booking_bool                                      0.0% \n",
      "date_time                                         0.0% \n",
      "site_id                                           0.0% \n",
      "srch_destination_id                               0.0% \n",
      "srch_length_of_stay                               0.0% \n",
      "srch_booking_window                               0.0% \n",
      "srch_adults_count                                 0.0% \n",
      "srch_children_count                               0.0% \n",
      "srch_room_count                                   0.0% \n",
      "srch_saturday_night_bool                          0.0% \n",
      "srch_query_affinity_score                         0.0% \n",
      "orig_destination_distance                         0.0% \n",
      "price_usd                                         0.0% \n",
      "promotion_flag                                    0.0% \n",
      "prop_country_id                                   0.0% \n",
      "prop_id                                           0.0% \n",
      "prop_starrating                                   0.0% \n",
      "prop_review_score                                 0.0% \n",
      "prop_brand_bool                                   0.0% \n",
      "prop_location_score1                              0.0% \n",
      "prop_location_score2                              0.0% \n",
      "prop_log_historical_price                         0.0% \n",
      "visitor_location_country_id                       0.0% \n",
      "visitor_hist_starrating                           0.0% \n",
      "visitor_hist_adr_usd                              0.0% \n",
      "month                                             0.0% \n",
      "day                                               0.0% \n",
      "hour                                              0.0% \n",
      "starrating_diff                                   0.0% \n",
      "country_id_match                                  0.0% \n",
      "user_usd_diff                                     0.0% \n",
      "has_booked_before                                 0.0% \n",
      "price_usd_srch_id                                 0.0% \n",
      "prop_review_score_srch_id                         0.0% \n",
      "prop_location_score1_srch_id                      0.0% \n",
      "prop_location_score2_srch_id                      0.0% \n",
      "prop_log_historical_price_srch_id                 0.0% \n",
      "price_usd_srch_destination_id                     0.0% \n",
      "prop_review_score_srch_destination_id             0.0% \n",
      "prop_location_score1_srch_destination_id          0.0% \n",
      "prop_location_score2_srch_destination_id          0.0% \n",
      "prop_log_historical_price_srch_destination_id     0.0% \n"
     ]
    }
   ],
   "source": [
    "# Check Percentage of NaN's per Column\n",
    "\n",
    "for column in df.columns:\n",
    "    print(\"{:50s}{:4.1%} {}\".format(column, np.mean(df[column].isnull()), \"NaN\" if df[column].isnull().any() else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df: pd.DataFrame, feature: str, with_respect_to: str):\n",
    "    ft = df[feature]\n",
    "    ft_mean = df.groupby(with_respect_to)[feature].transform('mean')\n",
    "    ft_std = df.groupby(with_respect_to)[feature].transform('std')\n",
    "    \n",
    "    ft_std[ft_std == 0] = 1    \n",
    "    return (ft - ft_mean) / ft_std\n",
    "\n",
    "\n",
    "#Normalize Numerical Features with Respect to Search ID, Property ID and Destination ID\n",
    "for norm_group in ['srch_id', 'srch_destination_id']:\n",
    "    for feature in ['price_usd', 'prop_review_score', 'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price']:\n",
    "        df[\"{}_{}\".format(feature, norm_group)] = normalize(df, feature, norm_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Random Fraction of Searches of DataFrame (To Speed Up Shit)\n",
    "# (This does take a while though...)\n",
    "RANDOM_FRACTION = 0.1\n",
    "selection = df.groupby('srch_id').filter(lambda x: np.random.uniform() < RANDOM_FRACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train and Test from random selection, again per Search ID\n",
    "TEST_TRAIN_SPLIT = 0.8\n",
    "\n",
    "unique_search_ids = np.unique(selection.srch_id)\n",
    "mask = np.random.uniform(0, 1, len(unique_search_ids)) < TEST_TRAIN_SPLIT\n",
    "\n",
    "train = selection[selection.srch_id.isin(unique_search_ids[mask])]\n",
    "test = selection[selection.srch_id.isin(unique_search_ids[~mask])]\n",
    "\n",
    "train_features = train.iloc[:, 'site_id':]\n",
    "train_labels = train['relevance']\n",
    "\n",
    "test_features = test.iloc[:, 'site_id':]\n",
    "test_labels = test['relevance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-cecfe629cc46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# This should be fairly quick now :) (30 seconds?, idk)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Predict Relevances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bram\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \"\"\"\n\u001b[0;32m    246\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bram\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bram\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 58\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train Regressor on Train Features and Train Labels (a.k.a. Relevances)\n",
    "# This should be fairly quick now :) (30 seconds?, idk)\n",
    "classifier = RandomForestRegressor()\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Predict Relevances\n",
    "predict_labels = classifier.predict(test_features)\n",
    "\n",
    "# Through Data to Calculate Score in new DataFrame\n",
    "# Note that predicted position = - predicted relevance\n",
    "result = pd.DataFrame({\n",
    "    'srch_id': test.srch_id,\n",
    "    'relevance': test.relevance,\n",
    "    'position': -predict_labels})\n",
    "\n",
    "print(\"Prediction:\", nDCG_mean(result))\n",
    "\n",
    "\n",
    "# Throw Random Positions in the Mix, to show we're at least doing better than Random :)\n",
    "result = pd.DataFrame({\n",
    "    'srch_id': test.srch_id,\n",
    "    'relevance': test.relevance,\n",
    "    'position': np.random.uniform(0, 1, len(test.relevance))\n",
    "})\n",
    "\n",
    "print(\"Random:\", nDCG_mean(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature Importances Graph\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.barh(np.arange(len(test_features.columns)), classifier.feature_importances_,\n",
    "       tick_label=test_features.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
