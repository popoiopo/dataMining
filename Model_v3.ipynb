{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nDCG(relevances: np.ndarray, positions: np.ndarray) -> float:\n",
    "    \"\"\"Compute Normalized Discounted Cumulative Gain based on:\n",
    "    - relevances: Numpy Array containing DCG Relevances (5 if booked, 1 if clicked)\n",
    "    - positions: Numpy Array containing Positions (The display order) \"\"\"\n",
    "    \n",
    "    positions_normalized = np.argsort(positions)\n",
    "    relevances_ordered = relevances[positions_normalized]\n",
    "    relevances_sorted = np.sort(relevances)[::-1]\n",
    "    \n",
    "    gain = 2 ** relevances_ordered - 1\n",
    "    ideal_gain = 2 ** relevances_sorted - 1\n",
    "    \n",
    "    discount = np.log2(np.arange(len(positions)) + 2)\n",
    "    \n",
    "    DCG = np.sum(gain / discount)\n",
    "    IDCG = np.sum(ideal_gain / discount)\n",
    "    \n",
    "    return DCG / IDCG\n",
    "\n",
    "\n",
    "def nDCG_mean(dataframe: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Mean Normalized Discounted Cumulative Gain on DataFrame\n",
    "    DataFrame must have fields: [srch_id, relevance, position]\n",
    "    \"\"\"\n",
    "    \n",
    "    nDCG_sum = 0.0\n",
    "    \n",
    "    searches = dataframe.groupby('srch_id')\n",
    "    \n",
    "    for name, search in searches:\n",
    "        nDCG_sum += nDCG(search.relevance.values, search.position.values)\n",
    "    return nDCG_sum / len(searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load That Shit (does take a while)\n",
    "df = pd.read_csv(\"data/training_set_VU_DM_2014.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Relevance Column to DataFrame\n",
    "relevance = np.zeros(len(df))\n",
    "relevance[df['click_bool'] == 1] = 1\n",
    "relevance[df['booking_bool'] == 1] = 5\n",
    "\n",
    "df['relevance'] = relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize DataFrame (a.k.a. throw out competitor info): handy for tweaking/overview?\n",
    "df = df[[\n",
    "    # ID\n",
    "    'srch_id',\n",
    "    \n",
    "    # Labels (to predict)\n",
    "    'position',\n",
    "    'relevance',\n",
    "    'click_bool',\n",
    "    'booking_bool',\n",
    "    \n",
    "    # Per Seach Features\n",
    "    'date_time',\n",
    "    'site_id',\n",
    "    'srch_destination_id',\n",
    "    'srch_length_of_stay',\n",
    "    'srch_booking_window',\n",
    "    'srch_adults_count',\n",
    "    'srch_children_count',\n",
    "    'srch_room_count',\n",
    "    'srch_saturday_night_bool',\n",
    "    'srch_query_affinity_score',\n",
    "    'orig_destination_distance',\n",
    "    \n",
    "    # Property Features\n",
    "    'price_usd',\n",
    "    'promotion_flag',\n",
    "    'prop_country_id',\n",
    "    'prop_id',\n",
    "    'prop_starrating',\n",
    "    'prop_review_score',\n",
    "    'prop_brand_bool',\n",
    "    'prop_location_score1',\n",
    "    'prop_location_score2',\n",
    "    'prop_log_historical_price',\n",
    "    \n",
    "    # Visitor Features\n",
    "    'visitor_location_country_id',\n",
    "    'visitor_hist_starrating',\n",
    "    'visitor_hist_adr_usd',\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean NaN's (The Pandas Way)\n",
    "df.srch_query_affinity_score = df.srch_query_affinity_score.fillna(df.srch_query_affinity_score.mean())\n",
    "df.orig_destination_distance = df.orig_destination_distance.fillna(df.orig_destination_distance.mean())\n",
    "\n",
    "df.prop_location_score2 = df.prop_location_score2.fillna(df.prop_location_score2.min())\n",
    "\n",
    "df.visitor_hist_adr_usd = df.visitor_hist_adr_usd.fillna(-1)\n",
    "df.visitor_hist_starrating = df.visitor_hist_starrating.fillna(-1)\n",
    "df.prop_review_score = df.prop_review_score.fillna(df.prop_review_score.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srch_id                                           0.0% \n",
      "position                                          0.0% \n",
      "relevance                                         0.0% \n",
      "click_bool                                        0.0% \n",
      "booking_bool                                      0.0% \n",
      "date_time                                         0.0% \n",
      "site_id                                           0.0% \n",
      "srch_destination_id                               0.0% \n",
      "srch_length_of_stay                               0.0% \n",
      "srch_booking_window                               0.0% \n",
      "srch_adults_count                                 0.0% \n",
      "srch_children_count                               0.0% \n",
      "srch_room_count                                   0.0% \n",
      "srch_saturday_night_bool                          0.0% \n",
      "srch_query_affinity_score                         0.0% \n",
      "orig_destination_distance                         0.0% \n",
      "price_usd                                         0.0% \n",
      "promotion_flag                                    0.0% \n",
      "prop_country_id                                   0.0% \n",
      "prop_id                                           0.0% \n",
      "prop_starrating                                   0.0% \n",
      "prop_review_score                                 0.0% \n",
      "prop_brand_bool                                   0.0% \n",
      "prop_location_score1                              0.0% \n",
      "prop_location_score2                              0.0% \n",
      "prop_log_historical_price                         0.0% \n",
      "visitor_location_country_id                       0.0% \n",
      "visitor_hist_starrating                           0.0% \n",
      "visitor_hist_adr_usd                              0.0% \n"
     ]
    }
   ],
   "source": [
    "# Check Percentage of NaN's per Column\n",
    "\n",
    "for column in df.columns:\n",
    "    print(\"{:50s}{:4.1%} {}\".format(column, np.mean(df[column].isnull()), \"NaN\" if df[column].isnull().any() else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Month/Day/Hour from Datetime\n",
    "dates = pd.DatetimeIndex(df['date_time'])\n",
    "\n",
    "df['month'] = dates.month\n",
    "df['day'] = dates.day\n",
    "df['hour'] = dates.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df: pd.DataFrame, feature: str, with_respect_to: str):\n",
    "    ft = df[feature]\n",
    "    ft_mean = df.groupby(with_respect_to)[feature].transform('mean')\n",
    "    ft_std = df.groupby(with_respect_to)[feature].transform('std')\n",
    "    \n",
    "    ft_std[ft_std == 0] = 1    \n",
    "    return (ft - ft_mean) / ft_std\n",
    "\n",
    "\n",
    "#Normalize Numerical Features with Respect to Search ID, Property ID and Destination ID and add those to the DataFrame\n",
    "for norm_group in ['srch_id', 'srch_destination_id']:\n",
    "    for feature in ['price_usd', 'prop_review_score', 'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price']:\n",
    "        df[\"{}_{}\".format(feature, norm_group)] = normalize(df, feature, norm_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_ranking'] = df.groupby('srch_id')['price_usd'].transform(lambda x: np.argsort(x) / len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test/Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Random Fraction of Searches of DataFrame (To Speed Up Shit)\n",
    "# (This does take a while though...)\n",
    "RANDOM_FRACTION = 0.1\n",
    "selection = df.groupby('srch_id').filter(lambda x: np.random.uniform() < RANDOM_FRACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93528 99579\n"
     ]
    }
   ],
   "source": [
    "# Split Train and Test from random selection, again per Search ID\n",
    "TEST_TRAIN_SPLIT = 0.8\n",
    "\n",
    "unique_search_ids = np.unique(selection.srch_id)\n",
    "mask = np.random.uniform(0, 1, len(unique_search_ids)) < TEST_TRAIN_SPLIT\n",
    "\n",
    "train = selection[selection.srch_id.isin(unique_search_ids[mask])]\n",
    "test = selection[selection.srch_id.isin(unique_search_ids[~mask])]\n",
    "\n",
    "# Downsample Training Set Negative Instances (Tip From Owen)\n",
    "DOWNSAMPLE = 0.2\n",
    "train = pd.concat([train[train.relevance > 0], train[train.relevance == 0].sample(frac=DOWNSAMPLE)])\n",
    "\n",
    "# Split Into Features and Labels\n",
    "train_features = train.loc[:, 'site_id':]\n",
    "train_labels = train['relevance']\n",
    "\n",
    "test_features = test.loc[:, 'site_id':]\n",
    "test_labels = test['relevance']\n",
    "\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train Regressor on Train Features w.r.t. Train Labels (a.k.a. Relevances)\n",
    "classifier = RandomForestRegressor()\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Predict Relevances\n",
    "predict_labels = classifier.predict(test_features)\n",
    "\n",
    "# Through Data to Calculate Score in new DataFrame\n",
    "# Note that predicted position = - predicted relevance\n",
    "result = pd.DataFrame({\n",
    "    'srch_id': test.srch_id,\n",
    "    'relevance': test.relevance,\n",
    "    'position': -predict_labels})\n",
    "\n",
    "print(\"Prediction:\", nDCG_mean(result))\n",
    "\n",
    "\n",
    "# Throw Random Positions in the Mix, to show we're at least doing better than Random :)\n",
    "result = pd.DataFrame({\n",
    "    'srch_id': test.srch_id,\n",
    "    'relevance': test.relevance,\n",
    "    'position': np.random.uniform(0, 1, len(test.relevance))\n",
    "})\n",
    "\n",
    "print(\"Random:\", nDCG_mean(result))\n",
    "\n",
    "# Plot Feature Importances Graph\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "fi_sort = np.argsort(classifier.feature_importances_)\n",
    "\n",
    "plt.barh(np.arange(len(test_features.columns)), classifier.feature_importances_[fi_sort],\n",
    "       tick_label=test_features.columns[fi_sort])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
